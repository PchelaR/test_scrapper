# TestScrapper

## 1. Что было сделано

Основные этапы:
1. Скачивание HTML-контента с каждой страницы сайта с помощью библиотеки `requests`.
2. Извлечение данных (цитаты, авторы, ссылки на авторов и теги) с использованием парсера `BeautifulSoup`.
3. Сохранение данных в формате JSON в отдельный файл.
4. Учёт пагинации на сайте (проверяет существование кнопки Next на странице).

## 2. Откуда были получены данные

Данные были получены с сайта [https://quotes.toscrape.com/](https://quotes.toscrape.com/).

- **Цитаты** — текст цитаты.
- **Авторы** — имена авторов цитат.
- **Ссылки на авторов** — ссылки на страницы с подробной информацией об авторах.
- **Теги** — перечень тегов, связанных с каждой цитатой.

## 3. Как осуществлялся сбор

### Технологии
- **Python**
- **Библиотека `requests`**
- **Библиотека `BeautifulSoup`**
- **Библиотека `python-dotenv`** (Для записи базовой части URL)

### Процесс сбора данных:
1. Отправляется запрос на первую страницу сайта, получаем HTML-контент.
2. Далее парсится HTML, и из него извлекаются нужные данные: цитаты, авторы, теги.
3. Проверяется наличие кнопки "Next" для перехода на следующую страницу.
4. Повторяется для каждой страницы, пока не будут собраны все данные (Пока имеется кнопка Next).
5. Все данные сохраняются в формате JSON для дальнейшего использования.

## 4. Почему был выбран тот или иной метод/инструмент

1. **requests** — Выбрана, так как она легковесная и проста в использовании
2. **BeautifulSoup** — Выбрана из за простоты и удобства для поиска и извлечения элементов на странице.
